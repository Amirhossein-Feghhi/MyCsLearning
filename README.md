#MyCsLearning

In this repo i will be share my daily challenges about SCIENCE.

## What is Web Crawler :
A web crawler is a program that collects content from the web. A web crawler finds web pages by
starting from a seed page and following links to find other pages, and following links from the other
pages it finds, and continuing to follow links until it has found many web pages.
Here is the process that a web crawler follows:
* Start from one preselected page("seed" page).
* Extract all the links on that page.
* Follow each of those links to find new pages.
* Extract all the links from all of the new pages found.
* Follow each of those links to find new pages.
* Extract all the links from all of the new pages found.


This keeps going as long as there are new pages to find, or until it is stopped.

Source : [Udacity](https://www.udacity.com)
